* GOES-18 / cloud testing

Spatial CIMIS Version 2 has a number of changes including;

- Cloud based GOES data fetching
- Simplified Rs calculation
- Python rewrites of the spatial cimis station data fetching
- Python rewrite of daily spatial-cimis product calculations

We need to test that these new applications are working. At the same time, we
need to review how much GOES-18 affected the production spatial cimis
values. Since GOES-18 started in 2023-01-04, we can run a comparison of the
previous year to see the changes and test the new setup.  We can do this most
easily by starting with DWR's spatial CIMIS data, and then comparing to this
data.

** Cloud-based GOES-18 vs. dish (GOES-18 bad calibration)

In order to run a more complete test on the suitability of using cloud sources
for goes data, and to look at the differences in GOES-18 data from the cloud
vs. from the dish (with the wrong calibration parameters)

To do this, we create a mapset that has all the historical Spatial CIMIS station
data.  Then, the historical radiation data, Rs,Rso,K and ETo are renamed with
the ~_dish~ suffix.  The we use the ~g.cimis.daily_solar~ to calculate new
Radiation parameters, and using these we recalculate ETo.  This allows us to
compare the outputs of the historical data, and the new cloud data, and know
that all changes are from the Rs changes only.

*** Production data retrieval
The first thing that is required is to get all the necessary data from the
current production processing.  I don't have access to DWR's data, but I
retrieved data from the UC Davis processing network.  To test the new (cloud)
based processing, we need to keep all the interpolated data the same, and drop
in replace the solar data.  So, I need to get a subset of the cimis data for
each day.  I created r.pack files for all the required data on Davis server.
The code for that looks like this:

You can see that I gave a 14 day startup time, so that starting on 2023-01-04,
when goes-18 went live, we had enough of a window for processing.

#+begin_src bash
  # One the UC Davis server, in grass
  vars='ea  es  ETo  K  Rnl  Rso  Rs  Tm  Tx  U2 Tn'
  for d in $(seq 0 655); do
      ymd=$(date --date="2022-12-20 + $d days" +%Y%m%d);
      for i in $vars; do
          echo $i@$ymd;
          r.pack --quiet input=$i@$ymd output=$ymd/$i.pack
      done;
      # Vectors as well
      for i in et z_normal; do
          v.pack input=$i@$ymd output=$ymd/$i.pack;
  done;
#+end_src

I then uploaded this data to a new project in google cloud.  The project
identifer is ~dwr-spatial-cimis~.  All in all this is about 85G of data.

#+begin_src bash
    gcloud config configurations list
    # shows
    #NAME               IS_ACTIVE  ACCOUNT             PROJECT              COMPUTE_DEFAULT_ZONE  COMPUTE_DEFAULT_REGION
    #dwr-spatial-cimis  True       qjhart@gmail.com    dwr-spatial-cimis
#+end_src

The packed data is located at ~gs://dwr-cimis-pack~.  Once you have the access
to this project, you can see the data with:

#+begin_src bash
  gsutil ls gs://dwr-cimis-pack
#+end_src

#+RESULTS:
| gs://dwr-cimis-pack/cimis-cloud/ |
| gs://dwr-cimis-pack/cimis-dish/  |

and you can fetch all the data for processing with:

#+begin_src bash
  gsutil -m rsync -r gs://dwr-cimis-pack/cimis-dish cimis-dish
#+end_src

*** Setting up your local machine
Once you have the packed data, you can unpack them locally, we know the names
for the vector files.  Also note that we know we want to compare the current
files for =Rs=,=K=, and =ETo= to new ones generated by the cloud data. So in
this step we move those data for later comparison.  Add the suffix =_dish= to
these original files (they are from the dish data).

#+begin_src bash
  for i in 202[34]*; do
      g.mapset -c mapset=$i;
      for r in cimis-dish/$i/*.pack; do
          if [[ $r =~ et || $r =~ z_normal ]]; then
              v.unpack input=$r;
          else
              r.unpack input=$r;
          fi;
      done;
      for r in ETo Rs K Rso Rnl; do
          g.rename --quiet  rast=${r},${r}_dish;
      done
  done
#+end_src

*** Cloud data processing

Now we need to create all the data for calculating the Rs using cloud based
goes-18 data. I don't have access to Symsoft's python code, but my
~g.cimis.daily_solar~ works just fine.  However, we'll also need to calculate
~Rnl~, and ~ETo~, given this new data.  This is also a symsoft python code, I
don't have, so I wrote a simple script to do this as well.  In that process, I
noticed that the ~gamma@500m~ file was missing, so I added that to the ~gdb~
database as well.  The script to do this is [[file:r.eto.sh][r.eto.sh]].

Finally, a batch script [[file:g.cimis.goes-batch]] combines the above steps to
allow the calculations to be run over a single mapset in a way that is simple to
script.  While running in grass, call this script for each mapset that you want
to run, eg.

#+begin_src bash
  start=2023-01-01
  for d in $(seq 0 5); do
      mapset=$(date --date="${start} + ${d} days" +%Y%m%d)
      g.mapset $mapset
      g.cimis.goes-batch --verbose
  done
  g.mapset PERMANENT
#+end_src

In practice, it takes about 20 minutes to run per day on my laptop.  I have
calculated these updated data though July 2023.

At this point, we could pack these *new* versions, and save them to the cloud
for others to test as well.

*** Comparison to dish data
There are a number of comparisons that you can now make.  For every day, you can
visually inspect and compare =Rs= to =Rs_dish=. =K= to =K_dish=, =ETo= to
=ETo_dish=.  Any differences are only from the change in Rs, since the rest of
the files haven't changed.  You can also calculate square differences in eah
mapset with:

#+begin_src bash
  for m in ETo Rs Rso K; do
      r.mapcalc --overwrite expression="${m}_rms=(${m}-${m}_dish)^2";
  done
#+end_src

These files are brighter where the differences are greater.  You can also look
at that differences over a longer time frame to see where the errors are
greatest in general.  The function looks something like:

#+begin_src bash
  r.mapcalc expression=Rs_rmse28='sqrt((Rs_rms@20230104 + Rs_rms@20230105 + Rs_rms@20230106 +\
Rs_rms@20230107 + Rs_rms@20230108 + Rs_rms@20230109 +\
Rs_rms@20230110 + Rs_rms@20230111 + Rs_rms@20230112 +\
Rs_rms@20230113 + Rs_rms@20230114 + Rs_rms@20230115 +\
Rs_rms@20230116 + Rs_rms@20230117 + Rs_rms@20230118 +\
Rs_rms@20230119 + Rs_rms@20230120 + Rs_rms@20230121 +\
Rs_rms@20230122 + Rs_rms@20230123 + Rs_rms@20230124 +\
Rs_rms@20230125 + Rs_rms@20230126 + Rs_rms@20230127 +\
Rs_rms@20230128 + Rs_rms@20230129 + Rs_rms@20230130 +\
Rs_rms@20230131) / 28)'
#+end_src

Again, the [[file:r.eto.sh][r.eto.sh]], can be made to run this summary.

*** Summary

**** 2023-01 through 2023-04
There were no large errors between the methods in this time frame.  If you did
look at the monthly =rmse28= values, however, there were some strange
anomolies.  Remember, the ~rmse28~ values are average error over 28 days, so
you'd expect errors to average out, and these images would be pretty smooth.
You get a different result than that.  This in in the GOES images.  However, by
202302-28, this seems to have disappeared from the data.

|------------------------------------+------------------------------------|
| [[file:images/20230131_Rs_rms228.jpg]] | [[file:images/20230228_Rs_rmse28.jpg]] |
| 2023-01-31 Rs RMSE 28 days         | 2023-02-28 Rs RMSE 28 days         |
#+ATTR_HTML: :width 150px

**** 2023-04-26 Image problems

Another problem started showing  up in April, where parts of  the B2 images were
not coming through on the cloud images.  The first example what on 2023-04-26 in
the  morning.  These  null values  affect the  ETo measurement  since the  nulls
propogate into the Rs values.  The nulls in the B2 value continues to affect the
ETo measurments until the  14 day albedo no longer includes  it.  This means the
next completly good ETo measurement doesn't come till 2023-05-11


|-------------------------------------+------------------------------+------------------------------|
| [[file:images/20230426_0641PST-B2.jpg]] | [[file:images/20230426_ETo.jpg]] | [[file:images/20230511_Eto.jpg]] |
| 2023-04-26 0641PST-B2               | 2023-04-26 ETo               | 2023-05-1 ETo                |
|                                     |                              |                              |
#+ATTR_HTML: :width 150px

The second example, was 2023-06-20 where image 0821PST-B2, was missing a
considerable portion.  This was not resolved until 2023-07-05.  This one is
actually worse since it affects most of California.

|-------------------------------------+------------------------------+------------------------------|
| [[file:images/20230620_0821PST-B2.jpg]] | [[file:images/20230620_ETo.jpg]] | [[file:images/20230705_ETo.jpg]] |
| 2023-06-20 0841PST-B2               | 2023-06-20 ETo               | 2023-07-05 ETo               |
#+ATTR_HTML: :width 150px


* Containerized services

To verify that cloud-based GOES data ingestion can be run platform agnostically,
a Dockerfile has been created for this testing.   that gets all required scripts, and prepares
them.  The Makefile shows the build command.  In addition, in preparation of
using this same image in a

** Locally
You can use image to run your processing as well.  The following method creates
a new container for each mapset run.  Looking at the Dockerfile, you can see
that there are two volume mounts, one temporary data, and one for the persistant
grass database.  This example shows a bind mount for the persistent data, while
allowing the temporary data to be saved to an emphemeral docker volume. This
assures they are not saved.  You could also bind the temporary directory if you
wanted to monitor the process more.

If LOCAL_USER_ID is set, then the container will make a new user with that given
uid.  This helps keep bind mounts with proper permissions, so you can use either
method interchangably.

#+begin_src bash
  make --directory=tests/goes18 build
  start=20230101
  for d in $(seq 0 5); do
      mapset=$(date --date="${start} + ${d} days" %Y%m%d)
      docker run --rm -e LOCAL_USER_ID=$(id --user) \
             -e MAPSET=$mapset -v ./gdb:/grassdb \
             localhost/dwr-spatial-cimis/goes18:1.0.0
  done
#+end_src

Processing has to go in order serially, so that the albedo is properly
maintained.
** Google Batch

Since these images are portable, you should also be able to use these pretty
easily in a cloud computing environment. As an example, the following
configuration shows this running using google batch. Here, the persistent grass
data is stored in in a google cloud bucket.  In a case like this, it's important
that the temporary files (and grass mapsets for reprojection) have been
sepatated, so that they are not thrashing the cloud bucket of persistant data.

The g.cimis.goes-batch has a special flag that has it look for the google
specific BATCH_TASK_INDEX parameter, so that mulitple days are run in a single
job.

An example configuration would look like:

#+name: job
#+begin_src json
  {
    "name": "projects/dwr-spatial-cimis/locations/us-west1/jobs/job-m319scal",
    "taskGroups": [
      {
        "taskCount": "5",
        "parallelism": "1",
        "taskSpec": {
          "computeResource": {
            "cpuMilli": "2000",
            "memoryMib": "4096"
          },
          "runnables": [
            {
              "container": {
                "imageUri": "us-west1-docker.pkg.dev/dwr-spatial-cimis/docker/goes18:1.0.0",
                "entrypoint": "",
                "volumes": [
                  "/mnt/disks/dwr-cimis-gdb:/grassdb:rw"
                ]
              },
              "environment": {
                "variables": {
                  "MAPSET": "20230101"
                }
              }
            }
          ],
          "volumes": [
            {
              "gcs": {
                "remotePath": "dwr-cimis-gdb"
              },
              "mountPath": "/mnt/disks/dwr-cimis-gdb"
            }
          ]
        }
      }
    ],
    "allocationPolicy": {
      "instances": [
        {
          "policy": {
            "provisioningModel": "STANDARD",
            "machineType": "e2-medium"
          }
        }
      ],
      "location": {
        "allowedLocations": [
          "zones/us-west1-a"
        ]
      }
    },
    "logsPolicy": {
      "destination": "CLOUD_LOGGING"
    }
  }
#+end_src


* Symsoft Testing
I do not have access to the python code devloped by symsoft, but the same
methodology should be used to verify their components as well.  Basically, this
should be done in a similar fashion, as above.  However, a good first step is to
compare *only* the differences from the spatially interpolated data.

So, for each mapset you might do this:

#+begin_src bash
  vars='ETo Tm Tn Tx U2 ea es'
  for i in 202[34]*; do
      g.mapset mapset=$i;
      for r in ${vars}; do
          g.rename --quiet  rast=${r},${r}_dish;
      done
  done
#+end_src

Then you can run the symsoft code to do the spatial interpolations, and as
above, compare the rasters in the mapset, that have been interpolated.  This is
using the vector data from the original/production mapset.  If you'd also like
to test the vector fetching you could do even more, (the choice of =xxxx= below
depends on if you want to do the test from the production code, for after
runnging the above Symsoft test).

#+begin_src bash
  vars='ETo Tm Tn Tx U2 ea es'
  vects='et z_normal'
  for i in 202[34]*; do
      g.mapset mapset=$i;
      for r in ${vars}; do
          g.rename --quiet  rast=${r},${r}_xxxx;
      done
      for v in ${vects}; do
          g.rename --quiet  vect=${v},${v}_xxxx;
      done
  done
#+end_src

Then you can refetch the station data. Some non-minor differences could occur in
this case, if the actual station data has changed since the data was first run.
