* GOES-18 / cloud testing

Spatial CIMIS Version 2 has a number of changes including;

- Cloud based GOES data fetching
- Simplified Rs calculation
- Python rewrites of the spatial cimis station data fetching
- Python rewrite of daily spatial-cimis product calculations

We need to test that these new applications are working. At the same time, we
need to review how much GOES-18 affected the production spatial cimis
values. Since GOES-18 started in 2023-01-04, we can run a comparison of the
previous year to see the changes and test the new setup.  We can do this most
easily by starting with DWR's spatial CIMIS data, and then comparing to this
data.

** Cloud-based GOES-18 testing

First we want to verify the cloud based Rs processing is working as expected.

*** Production data retrieval
The first thing that is required is to get all the necessary data from the
current production processing.  I don't have access to DWR's data, but I
retrieved data from the UC Davis processing network.  To test the new (cloud)
based processing, we need to keep all the interpolated data the same, and drop
in replace the solar data.  So, I need to get a subset of the cimis data for
each day.  I created r.pack files for all the required data on Davis server.
The code for that looks like this:

You can see that I gave a 14 day startup time, so that starting on 2023-01-04,
when goes-18 went live, we had enough of a window for processing.

#+begin_src bash
  # One the UC Davis server, in grass
  vars='ea  es  ETo  K  Rnl  Rso  Rs  Tm  Tx  U2 Tn'
  for d in $(seq 0 655); do
      ymd=$(date --date="2022-12-20 + $d days" +%Y%m%d);
      for i in $vars; do
          echo $i@$ymd;
          r.pack --quiet input=$i@$ymd output=$ymd/$i.pack
      done;
      # Vectors as well
      for i in et z_normal; do
          v.pack input=$i@$ymd output=$ymd/$i.pack;
  done;
#+end_src

I then uploaded this data to a new project in google cloud.  The project
identifer is ~dwr-spatial-cimis~.  All in all this is about 85G of data.

#+begin_src bash
    gcloud config configurations list
    # shows
    #NAME               IS_ACTIVE  ACCOUNT             PROJECT              COMPUTE_DEFAULT_ZONE  COMPUTE_DEFAULT_REGION
    #dwr-spatial-cimis  True       qjhart@gmail.com    dwr-spatial-cimis
#+end_src

The packed data is located at ~gs://dwr-cimis-pack~.  Once you have the access
to this project, you can see the data with:

#+begin_src bash
  gsutil ls gs://dwr-cimis-pack
#+end_src

#+RESULTS:
| gs://dwr-cimis-pack/cimis-cloud/ |
| gs://dwr-cimis-pack/cimis-dish/  |

and you can fetch all the data for processing with:

#+begin_src bash
  gsutil -m rsync -r gs://dwr-cimis-pack/cimis-dish cimis-dish
#+end_src

*** Setting up your local machine

Once you have the packed data, you can unpack them locally, we know the names
for the vector files.  Also note that we know we want to compare the current
files for =Rs=,=K=, and =ETo= to new ones generated by the cloud data. So in
this step we move those data for later comparison.  Add the suffix =_dish= to
these original files (they are from the dish data).

#+begin_src bash
  for i in 202[34]*; do
      g.mapset -c mapset=$i;
      for r in cimis-dish/$i/*.pack; do
          if [[ $r =~ et || $r =~ z_normal ]]; then
              v.unpack input=$r;
          else
              r.unpack input=$r;
          fi;
      done;
      for r in ETo Rs K Rso Rnl; do
          g.rename --quiet  rast=${r},${r}_dish;
      done
  done
#+end_src

*** Cloud data processing

Now we need to create all the data for calculating the Rs using cloud based
goes-18 data. I don't have access to Symsoft's python code, but my
~g.cimis.daily_solar~ works just fine.  However, we'll also need to calculate
~Rnl~, and ~ETo~, given this new data.  This is also a symsoft python code, I
don't have, so I wrote a simple script to do this as well.  In that process, I
noticed that the ~gamma@500m~ file was missing, so I added that to the ~gdb~
database as well.  The script to do this is [[file:r.eto.sh][r.eto.sh]].

The command line to run these would be something like this, if you were running
in this directory.

#+begin_src bash
  start='2023-12-22'
  days=650
  for d in $(seq 0 $days); do
      ymd=$(date --date="2023-04-02 + $d days" +%Y%m%d);
      g.mapset $ymd;
      ~/g.cimis.daily_solar/g.cimis.daily_solar.sh;
      ./r.eto.sh;
  done;
  g.mapset quinn
#+end_src

In practice, it takes about 20 minutes to run this, and since it's on my laptop,
I've only done a few months. I will look to move this to a cloud computing
environment for completeness.

At this point, we could pack these *new* versions, and save them to the cloud
for others to test as well.

*** Comparison to dish data

There are a number of comparisons that you can now make.  For every day, you can
visually inspect and compare =Rs= to =Rs_dish=. =K= to =K_dish=, =ETo= to
=ETo_dish=.  Any differences are only from the change in Rs, since the rest of
the files haven't changed.  You can also calculate square differences in eah
mapset with:

#+begin_src bash
  for m in ETo Rs Rso K; do
      r.mapcalc --overwrite expression="${m}_rms=(${m}-${m}_dish)^2";
  done
#+end_src

These files are brighter where the differences are greater.  You can also look
at that differences over a longer time frame to see where the errors are
greatest in general.  The function looks something like:

#+begin_src bash
  r.mapcalc expression=Rs_rmse28='sqrt((Rs_rms@20230104 + Rs_rms@20230105 + Rs_rms@20230106 +\
Rs_rms@20230107 + Rs_rms@20230108 + Rs_rms@20230109 +\
Rs_rms@20230110 + Rs_rms@20230111 + Rs_rms@20230112 +\
Rs_rms@20230113 + Rs_rms@20230114 + Rs_rms@20230115 +\
Rs_rms@20230116 + Rs_rms@20230117 + Rs_rms@20230118 +\
Rs_rms@20230119 + Rs_rms@20230120 + Rs_rms@20230121 +\
Rs_rms@20230122 + Rs_rms@20230123 + Rs_rms@20230124 +\
Rs_rms@20230125 + Rs_rms@20230126 + Rs_rms@20230127 +\
Rs_rms@20230128 + Rs_rms@20230129 + Rs_rms@20230130 +\
Rs_rms@20230131) / 28)'
#+end_src

Again, the [[file:r.eto.sh][r.eto.sh]], can be made to run this summary.

*** Summary

There have been no large errors between the methods for 202301, 202302, and
202303, so thing are looking fine.  Looking at the monthly =rmse28= do show some
strange anomolies, that are not important, but need to be tracked down.

** Symsoft Testing

I do not have access to the python code devloped by symsoft, but the same
methodology should be used to verify their components as well.  Basically, this
should be done in a similar fashion, as above.  However, a good first step is to
compare *only* the differences from the spatially interpolated data.

So, for each mapset you might do this:

#+begin_src bash
  vars='ETo Tm Tn Tx U2 ea es'
  for i in 202[34]*; do
      g.mapset mapset=$i;
      for r in ${vars}; do
          g.rename --quiet  rast=${r},${r}_dish;
      done
  done
#+end_src

Then you can run the symsoft code to do the spatial interpolations, and as
above, compare the rasters in the mapset, that have been interpolated.  This is
using the vector data from the original/production mapset.  If you'd also like
to test the vector fetching you could do even more, (the choice of =xxxx= below
depends on if you want to do the test from the production code, for after
runnging the above Symsoft test).

#+begin_src bash
  vars='ETo Tm Tn Tx U2 ea es'
  vects='et z_normal'
  for i in 202[34]*; do
      g.mapset mapset=$i;
      for r in ${vars}; do
          g.rename --quiet  rast=${r},${r}_xxxx;
      done
      for v in ${vects}; do
          g.rename --quiet  vect=${v},${v}_xxxx;
      done
  done
#+end_src

Then you can refetch the station data. Some non-minor differences could occur in
this case, if the actual station data has changed since the data was first run.
